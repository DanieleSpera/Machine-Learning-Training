{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Prediction\n",
    "2D Layers CNN & Pool\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credits:\n",
    "- https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/cifar-cnn/cifar10_cnn_solution.ipynb\n",
    "- Balakrishnan, Anusha, and Alisha Rege. \n",
    "Reading Emotions from Speech Using Deep Neural Networks. \n",
    "Stanford, 2017, pp. 1â€“8, Reading Emotions from Speech Using Deep Neural Networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Scipy\n",
    "import scipy\n",
    "\n",
    "#Numpy\n",
    "import numpy as np\n",
    "\n",
    "#Pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Librosa\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Other  \n",
    "import os\n",
    "\n",
    "# Local\n",
    "from source_pytorch.newDataset import NewDatset\n",
    "from source_pytorch.newModel import NewModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the parameters and load the data\n",
    "setting = {\n",
    "  \"audio_duration\"  :   4,\n",
    "  \"n_mfcc\"          :   40,\n",
    "  \"sampling_rate\"   :   44100,\n",
    "  \"audio_duration\"  :   4,\n",
    "  \"number_samplig\"  :   354,\n",
    "  \"dataroot\"        :   \".\",\n",
    "  \"batch_size\"      :   16,\n",
    "  \"epochs\"          :   50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>fname</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fearful</td>\n      <td>03-01-06-01-02-02-02.wav</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>angry</td>\n      <td>03-01-05-01-02-01-16.wav</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>surprise</td>\n      <td>03-01-08-01-01-01-14.wav</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fearful</td>\n      <td>03-01-06-01-02-02-16.wav</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>angry</td>\n      <td>03-01-05-01-02-01-02.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      label                     fname\n0   fearful  03-01-06-01-02-02-02.wav\n1     angry  03-01-05-01-02-01-16.wav\n2  surprise  03-01-08-01-01-01-14.wav\n3   fearful  03-01-06-01-02-02-16.wav\n4     angry  03-01-05-01-02-01-02.wav"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the reference\n",
    "ref = pd.read_csv(\"./ReferenceData.csv\")#[:200]\n",
    "#Print head / test successfull load\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Dataset Len:\t Train - 810 \tValid Dataset - 270 \tTest Dataset - 360\n"
    }
   ],
   "source": [
    "#Split the Train and Test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(ref[\"fname\"]\n",
    "                                                    ,ref[\"label\"]\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=50\n",
    "                                                   )\n",
    "#Split the Train and Validation dataset\n",
    "X_train, X_valid , y_train, y_valid = train_test_split(X_train\n",
    "                                                    , y_train\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=50\n",
    "                                                   )\n",
    "\n",
    "\n",
    "#Merge data and prediction in 1 dataset for train validation and test\n",
    "train_df = pd.concat([X_train,y_train], axis = 1)  \n",
    "valid_df =  pd.concat([X_valid,y_valid], axis = 1)\n",
    "test_df = pd.concat([X_test,y_test], axis = 1)\n",
    "\n",
    "print('Dataset Len:\\t Train - {} \\tValid Dataset - {} \\tTest Dataset - {}'.format(len(train_df), len(valid_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compose DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    NewDatset(\n",
    "        dataframe=train_df,\n",
    "        n_melspec = setting[\"n_mfcc\"], \n",
    "        sampling_rate = setting[\"sampling_rate\"], \n",
    "        audio_duration= setting[\"audio_duration\"], \n",
    "        number_samples= setting[\"number_samplig\"]\n",
    "        ), \n",
    "    batch_size= setting[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    NewDatset(\n",
    "        dataframe=valid_df,\n",
    "        n_melspec = setting[\"n_mfcc\"], \n",
    "        sampling_rate = setting[\"sampling_rate\"], \n",
    "        audio_duration= setting[\"audio_duration\"], \n",
    "        number_samples= setting[\"number_samplig\"]\n",
    "    ), \n",
    "    batch_size=setting[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "NewModel(\n  (conv): Conv2d(1, 1280, kernel_size=(3, 3), stride=(1, 1))\n  (batch): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc): Linear(in_features=4158720, out_features=8, bias=True)\n  (drop): Dropout2d(p=0.2, inplace=False)\n)\n"
    }
   ],
   "source": [
    "#Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = NewModel()\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch: 1 \tTraining Loss: 6462.311776 \tValidation Loss: 9244.208746\nValidation loss decreased (inf --> 9244.208746).  Saving model ...\nEpoch: 2 \tTraining Loss: 3205.553911 \tValidation Loss: 4509.329225\nValidation loss decreased (9244.208746 --> 4509.329225).  Saving model ...\nEpoch: 3 \tTraining Loss: 1664.547634 \tValidation Loss: 4046.533096\nValidation loss decreased (4509.329225 --> 4046.533096).  Saving model ...\nEpoch: 4 \tTraining Loss: 1183.308363 \tValidation Loss: 1717.169438\nValidation loss decreased (4046.533096 --> 1717.169438).  Saving model ...\nEpoch: 5 \tTraining Loss: 889.894900 \tValidation Loss: 2297.971812\nEpoch: 6 \tTraining Loss: 613.600841 \tValidation Loss: 1873.949158\nEpoch: 7 \tTraining Loss: 483.143839 \tValidation Loss: 1336.064195\nValidation loss decreased (1717.169438 --> 1336.064195).  Saving model ...\nEpoch: 8 \tTraining Loss: 351.855316 \tValidation Loss: 647.231279\nValidation loss decreased (1336.064195 --> 647.231279).  Saving model ...\nEpoch: 9 \tTraining Loss: 250.894140 \tValidation Loss: 962.872582\nEpoch: 10 \tTraining Loss: 186.050520 \tValidation Loss: 656.184248\nEpoch: 11 \tTraining Loss: 140.254325 \tValidation Loss: 464.519201\nValidation loss decreased (647.231279 --> 464.519201).  Saving model ...\nEpoch: 12 \tTraining Loss: 114.385918 \tValidation Loss: 298.829590\nValidation loss decreased (464.519201 --> 298.829590).  Saving model ...\nEpoch: 13 \tTraining Loss: 76.116892 \tValidation Loss: 265.337062\nValidation loss decreased (298.829590 --> 265.337062).  Saving model ...\nEpoch: 14 \tTraining Loss: 73.513395 \tValidation Loss: 310.323602\nEpoch: 15 \tTraining Loss: 52.487581 \tValidation Loss: 192.784168\nValidation loss decreased (265.337062 --> 192.784168).  Saving model ...\nEpoch: 16 \tTraining Loss: 33.292625 \tValidation Loss: 202.475659\nEpoch: 17 \tTraining Loss: 34.151824 \tValidation Loss: 106.787048\nValidation loss decreased (192.784168 --> 106.787048).  Saving model ...\nEpoch: 18 \tTraining Loss: 27.466278 \tValidation Loss: 110.839100\nEpoch: 19 \tTraining Loss: 17.890698 \tValidation Loss: 80.699298\nValidation loss decreased (106.787048 --> 80.699298).  Saving model ...\nEpoch: 20 \tTraining Loss: 12.582910 \tValidation Loss: 76.388537\nValidation loss decreased (80.699298 --> 76.388537).  Saving model ...\nEpoch: 21 \tTraining Loss: 11.527653 \tValidation Loss: 75.052714\nValidation loss decreased (76.388537 --> 75.052714).  Saving model ...\nEpoch: 22 \tTraining Loss: 8.378807 \tValidation Loss: 56.380010\nValidation loss decreased (75.052714 --> 56.380010).  Saving model ...\nEpoch: 23 \tTraining Loss: 7.971705 \tValidation Loss: 39.627748\nValidation loss decreased (56.380010 --> 39.627748).  Saving model ...\nEpoch: 24 \tTraining Loss: 8.079675 \tValidation Loss: 34.757155\nValidation loss decreased (39.627748 --> 34.757155).  Saving model ...\nEpoch: 25 \tTraining Loss: 4.450210 \tValidation Loss: 25.032314\nValidation loss decreased (34.757155 --> 25.032314).  Saving model ...\nCPU times: user 5h 4min 31s, sys: 3h 19min 34s, total: 8h 24min 5s\nWall time: 1d 21h 25min 2s\n"
    }
   ],
   "source": [
    "%%time\n",
    "#Credits: https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/cifar-cnn/cifar10_cnn_solution.ipynb\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, setting[\"epochs\"]+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_dataloader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_dataloader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_dataloader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_dataloader.sampler)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_cifar.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "# Load Test Data\n",
    "test_dataloader = DataLoader(\n",
    "    NewDatset(\n",
    "        dataframe=test_df,\n",
    "        n_melspec = setting[\"n_mfcc\"], \n",
    "        sampling_rate = setting[\"sampling_rate\"], \n",
    "        audio_duration= setting[\"audio_duration\"], \n",
    "        number_samples= setting[\"number_samplig\"]\n",
    "        ), \n",
    "    batch_size= setting[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Test Loss: 24.902299\n\nTest Accuracy of angry: N/A (no training examples)\nTest Accuracy of surprise: 20% ( 9/45)\nTest Accuracy of fearful:  6% ( 3/47)\nTest Accuracy of neutral:  1% ( 1/51)\nTest Accuracy of  calm: 16% ( 9/54)\nTest Accuracy of happy: 10% ( 4/40)\nTest Accuracy of   sad:  9% ( 3/33)\nTest Accuracy of disgust:  2% ( 1/47)\nTest Total Accuracy  9%\n"
    }
   ],
   "source": [
    "# Get the model and evaluate\n",
    "model.eval()\n",
    "\n",
    "right_pred = 0\n",
    "total_pred = 0\n",
    "\n",
    "for data, target in test_dataloader:\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "\n",
    "    #for i in range(setting[\"batch_size\"]):\n",
    "    for i in range(len(target.data)):\n",
    "        if target.data[i]:\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "    total_pred += target.size(0)\n",
    "    right_pred += (pred == target).sum().item()\n",
    "        \n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_dataloader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "classes = ['angry', 'surprise', 'fearful', 'neutral', 'calm', 'happy', 'sad', 'disgust']\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('Test Total Accuracy %2d%%' % (100 * right_pred / total_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bit397be61b2f12438a83a89fe139871e40"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}