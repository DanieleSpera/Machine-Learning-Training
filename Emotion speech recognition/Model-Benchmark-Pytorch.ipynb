{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Benckmark Prediction\n",
    "Linear Regression Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credits:\n",
    "- https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Scipy\n",
    "import scipy\n",
    "\n",
    "#Numpy\n",
    "import numpy as np\n",
    "\n",
    "#Pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Librosa\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Other  \n",
    "import os\n",
    "\n",
    "# Local\n",
    "from source_pytorch.LinearDataset import LinearDatset\n",
    "from source_pytorch.LinearModel import LinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the parameters and load the data\n",
    "setting = {\n",
    "  \"audio_duration\"  :   4,\n",
    "  \"n_mfcc\"          :   40,\n",
    "  \"sampling_rate\"   :   44100,\n",
    "  \"audio_duration\"  :   4,\n",
    "  \"number_samplig\"  :   354,\n",
    "  \"dataroot\"        :   \".\",\n",
    "  \"batch_size\"      :   10,\n",
    "  \"epochs\"          :   50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>fname</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fearful</td>\n      <td>03-01-06-01-02-02-02.wav</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>angry</td>\n      <td>03-01-05-01-02-01-16.wav</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>surprise</td>\n      <td>03-01-08-01-01-01-14.wav</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fearful</td>\n      <td>03-01-06-01-02-02-16.wav</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>angry</td>\n      <td>03-01-05-01-02-01-02.wav</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      label                     fname\n0   fearful  03-01-06-01-02-02-02.wav\n1     angry  03-01-05-01-02-01-16.wav\n2  surprise  03-01-08-01-01-01-14.wav\n3   fearful  03-01-06-01-02-02-16.wav\n4     angry  03-01-05-01-02-01-02.wav"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the reference\n",
    "ref = pd.read_csv(\"./ReferenceData.csv\")#[:37]\n",
    "#Print head / test successfull load\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Dataset Len:\t Train - 810 \tValid Dataset - 270 \tTest Dataset - 360\n"
    }
   ],
   "source": [
    "#Split the Train and Test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(ref[\"fname\"]\n",
    "                                                    ,ref[\"label\"]\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=50\n",
    "                                                   )\n",
    "#Split the Train and Validation dataset\n",
    "X_train, X_valid , y_train, y_valid = train_test_split(X_train\n",
    "                                                    , y_train\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=50\n",
    "                                                   )\n",
    "\n",
    "\n",
    "#Merge data and prediction in 1 dataset for train validation and test\n",
    "train_df = pd.concat([X_train,y_train], axis = 1)  \n",
    "valid_df =  pd.concat([X_valid,y_valid], axis = 1)\n",
    "test_df = pd.concat([X_test,y_test], axis = 1)\n",
    "\n",
    "print('Dataset Len:\\t Train - {} \\tValid Dataset - {} \\tTest Dataset - {}'.format(len(train_df), len(valid_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compose DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    LinearDatset(\n",
    "        dataframe=train_df,\n",
    "        n_melspec = setting[\"n_mfcc\"], \n",
    "        sampling_rate = setting[\"sampling_rate\"], \n",
    "        audio_duration= setting[\"audio_duration\"], \n",
    "        number_samples= setting[\"number_samplig\"]\n",
    "        ), \n",
    "    batch_size= setting[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    LinearDatset(\n",
    "        dataframe=valid_df,\n",
    "        n_melspec = setting[\"n_mfcc\"], \n",
    "        sampling_rate = setting[\"sampling_rate\"], \n",
    "        audio_duration= setting[\"audio_duration\"], \n",
    "        number_samples= setting[\"number_samplig\"]\n",
    "    ), \n",
    "    batch_size=setting[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "LinearModel(\n  (linear): Linear(in_features=13800, out_features=8, bias=True)\n)\n"
    }
   ],
   "source": [
    "#Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LinearModel()\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n Epoch: 1 \tTraining Loss: 5069.969673 \tValidation Loss: 5907.600188\nValidation loss decreased (inf --> 5907.600188).  Saving model ...\n\n Epoch: 2 \tTraining Loss: 3761.992362 \tValidation Loss: 2157.638645\nValidation loss decreased (5907.600188 --> 2157.638645).  Saving model ...\n\n Epoch: 3 \tTraining Loss: 2540.186466 \tValidation Loss: 3884.556302\n\n Epoch: 4 \tTraining Loss: 1969.485092 \tValidation Loss: 2658.230613\n\n Epoch: 5 \tTraining Loss: 2342.914899 \tValidation Loss: 5483.300302\n\n Epoch: 6 \tTraining Loss: 3168.877873 \tValidation Loss: 4568.887533\n\n Epoch: 7 \tTraining Loss: 2433.802193 \tValidation Loss: 3487.747649\n\n Epoch: 8 \tTraining Loss: 2810.761915 \tValidation Loss: 3656.353674\n\n Epoch: 9 \tTraining Loss: 1818.549708 \tValidation Loss: 2972.077361\n\n Epoch: 10 \tTraining Loss: 2220.330833 \tValidation Loss: 3063.745949\n\n Epoch: 11 \tTraining Loss: 2369.596281 \tValidation Loss: 3391.804091\n\n Epoch: 12 \tTraining Loss: 2521.363591 \tValidation Loss: 6348.427915\n\n Epoch: 13 \tTraining Loss: 2277.213622 \tValidation Loss: 2233.344620\n\n Epoch: 14 \tTraining Loss: 2720.823567 \tValidation Loss: 4808.937459\n\n Epoch: 15 \tTraining Loss: 2330.640168 \tValidation Loss: 2680.854673\n\n Epoch: 16 \tTraining Loss: 2600.926521 \tValidation Loss: 4973.454463\n\n Epoch: 17 \tTraining Loss: 2830.814304 \tValidation Loss: 4682.862657\n\n Epoch: 18 \tTraining Loss: 2941.625749 \tValidation Loss: 5167.483480\n\n Epoch: 19 \tTraining Loss: 2221.589130 \tValidation Loss: 2499.092724\n\n Epoch: 20 \tTraining Loss: 2445.128717 \tValidation Loss: 2647.024699\n\n Epoch: 21 \tTraining Loss: 2490.687721 \tValidation Loss: 3941.183612\n\n Epoch: 22 \tTraining Loss: 2152.894712 \tValidation Loss: 3424.007718\n\n Epoch: 23 \tTraining Loss: 2314.726851 \tValidation Loss: 3048.201588\n\n Epoch: 24 \tTraining Loss: 2379.243549 \tValidation Loss: 4199.296414\n\n Epoch: 25 \tTraining Loss: 2463.226832 \tValidation Loss: 4108.344406\n\n Epoch: 26 \tTraining Loss: 2487.835063 \tValidation Loss: 3344.704852\n\n Epoch: 27 \tTraining Loss: 2544.741703 \tValidation Loss: 5759.908411\n\n Epoch: 28 \tTraining Loss: 2721.561495 \tValidation Loss: 3300.594641\n\n Epoch: 29 \tTraining Loss: 2416.409643 \tValidation Loss: 2239.188685\n\n Epoch: 30 \tTraining Loss: 1650.330559 \tValidation Loss: 4602.894676\n\n Epoch: 31 \tTraining Loss: 2605.694755 \tValidation Loss: 3217.696122\n\n Epoch: 32 \tTraining Loss: 2380.539393 \tValidation Loss: 3383.840798\n\n Epoch: 33 \tTraining Loss: 2197.186876 \tValidation Loss: 3560.934905\n\n Epoch: 34 \tTraining Loss: 2052.193702 \tValidation Loss: 4547.886158\n\n Epoch: 35 \tTraining Loss: 2889.900853 \tValidation Loss: 6868.523058\n\n Epoch: 36 \tTraining Loss: 2249.836449 \tValidation Loss: 3701.995162\n\n Epoch: 37 \tTraining Loss: 2749.610529 \tValidation Loss: 3621.490551\n\n Epoch: 38 \tTraining Loss: 2063.177559 \tValidation Loss: 3273.805461\n\n Epoch: 39 \tTraining Loss: 2604.784921 \tValidation Loss: 5581.500298\n\n Epoch: 40 \tTraining Loss: 2633.291476 \tValidation Loss: 4398.848746\n\n Epoch: 41 \tTraining Loss: 2762.925045 \tValidation Loss: 3703.896118\n\n Epoch: 42 \tTraining Loss: 2350.040042 \tValidation Loss: 4733.005660\n\n Epoch: 43 \tTraining Loss: 2114.967442 \tValidation Loss: 3117.723488\n\n Epoch: 44 \tTraining Loss: 2269.958135 \tValidation Loss: 3815.726382\n\n Epoch: 45 \tTraining Loss: 2349.272494 \tValidation Loss: 3301.988833\n\n Epoch: 46 \tTraining Loss: 2122.970958 \tValidation Loss: 4336.733064\n\n Epoch: 47 \tTraining Loss: 2537.117047 \tValidation Loss: 2742.092525\n\n Epoch: 48 \tTraining Loss: 2275.850557 \tValidation Loss: 5443.578274\n\n Epoch: 49 \tTraining Loss: 2159.207700 \tValidation Loss: 3329.173182\n\n Epoch: 50 \tTraining Loss: 2222.113445 \tValidation Loss: 3164.667376\nCPU times: user 1h 41min 35s, sys: 8min 46s, total: 1h 50min 21s\nWall time: 1h 19min 32s\n"
    }
   ],
   "source": [
    "%%time\n",
    "#Credits: https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/cifar-cnn/cifar10_cnn_solution.ipynb\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "\n",
    "for epoch in range(1, setting[\"epochs\"]+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_dataloader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_dataloader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_dataloader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_dataloader.sampler)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('\\n Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_cifar.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "# Load Test Data\n",
    "test_dataloader = DataLoader(\n",
    "    LinearDatset(\n",
    "        dataframe=test_df,\n",
    "        n_melspec = setting[\"n_mfcc\"], \n",
    "        sampling_rate = setting[\"sampling_rate\"], \n",
    "        audio_duration= setting[\"audio_duration\"], \n",
    "        number_samples= setting[\"number_samplig\"]\n",
    "        ), \n",
    "    batch_size= setting[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=0)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Test Loss: 2346.813819\n\nTest Accuracy of angry: 32% (14/43)\nTest Accuracy of surprise:  0% ( 0/45)\nTest Accuracy of fearful:  0% ( 0/47)\nTest Accuracy of neutral:  0% ( 0/51)\nTest Accuracy of  calm:  0% ( 0/54)\nTest Accuracy of happy: 37% (15/40)\nTest Accuracy of   sad:  0% ( 0/33)\nTest Accuracy of disgust:  0% ( 0/47)\nTest Total Accuracy  8%\n"
    }
   ],
   "source": [
    "# Get the model and evaluate\n",
    "model.eval()\n",
    "\n",
    "right_pred = 0\n",
    "total_pred = 0\n",
    "\n",
    "for data, target in test_dataloader:\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    _, pred = torch.max(output, 1)\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "\n",
    "    for i in range(setting[\"batch_size\"]):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "    total_pred += target.size(0)\n",
    "    right_pred += (pred == target).sum().item()\n",
    "        \n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_dataloader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "classes = ['angry', 'surprise', 'fearful', 'neutral', 'calm', 'happy', 'sad', 'disgust']\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('Test Total Accuracy %2d%%' % (100 * right_pred / total_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bit397be61b2f12438a83a89fe139871e40"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}